# Environment Variables Configuration
# Copy this file to .env and customize the values

# ============================================================================
# Model Configuration
# ============================================================================
# Path to the trained model directory
# Default: ./models/symptom_classifier-old-trainer/final/
MODEL_PATH=./models/symptom_classifier-old-trainer/final/

# Alternative model paths you might use:
# MODEL_PATH=./models/symptom_classifier-mini/final/
# MODEL_PATH=/absolute/path/to/custom/model/

# ============================================================================
# LLM Backend Configuration
# ============================================================================
# LLM backend to use for clinical summaries (required for CLI and Chainlit apps)
# Options: ollama, lightning
# Default: ollama
LLM_BACKEND=ollama

# ----------------------------------------------------------------------------
# Ollama Configuration (local LLM server)
# ----------------------------------------------------------------------------
# Ollama server URL
# Default: http://localhost:11434
# OLLAMA_BASE_URL=http://localhost:11434

# Ollama model name
# Default: llama3
# Common options: llama3, llama2, mistral, mixtral
# OLLAMA_MODEL=llama3

# ----------------------------------------------------------------------------
# Lightning AI Configuration (cloud LLM service)
# ----------------------------------------------------------------------------
# Lightning AI API base URL
# Default: https://lightning.ai/api/v1/
# LIGHTNING_BASE_URL=https://lightning.ai/api/v1/

# Lightning AI model name
# Default: lightning-ai/llama-3.3-70b
# LIGHTNING_MODEL=lightning-ai/llama-3.3-70b

# Lightning AI API key (REQUIRED if using lightning backend)
# Get your API key from: https://lightning.ai
# LIGHTNING_API_KEY=your-api-key-here
